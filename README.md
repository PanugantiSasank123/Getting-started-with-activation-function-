# Getting-started-with-activation-function-


Dataset (MNIST / Fashion MNIST):
Description: Handwritten digits or clothing images for classification tasks.

Activation Functions:

1. ReLU: 
   - Rectified linear unit; outputs input or zero, non-linear activation.
   
2. Sigmoid: 
   - S-shaped curve; maps inputs between 0 and 1 for probabilities.

3. ELU: 
   - Exponential Linear Unit; smooth negative outputs, improves learning dynamics.

4. SELU: 
   - Scaled ELU; self-normalizing, used in deep networks for stable outputs.

5. GELU: 
   - Gaussian Error Linear Unit; approximates Gaussian distribution for smoother activations.

6. Tanh: 
   - Hyperbolic tangent; outputs between -1 and 1, better for zero-centered data.
